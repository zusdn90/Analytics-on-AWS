{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analytics On AWS workshop\n",
    "\n",
    "Take your time to read through the instructions provided in this notebook.\n",
    "\n",
    "###### Learning Objectives\n",
    "- Understand how to interactively author Glue ETL scripts using Glue Dev Endpoints & SageMaker notebooks\n",
    "- Use boto3 to call Glue APIs to do Glue administrative and operational activities\n",
    "\n",
    "**Execute the code blocks one cell at a time**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Import Libraries \n",
    "- In this notebook we will be using the following classes, here are some of the important ones\n",
    "    - SparkContext - Main entry point for Spark functionality. A SparkContext represents the connection to a Spark cluster, and can be used to create RDDs, accumulators and broadcast variables on that cluster.\n",
    "    - GlueContext - Wraps the Apache SparkSQL SQLContext object, and thereby provides mechanisms for interacting with the Apache Spark platform\n",
    "    - boto3 - AWS's Python SDK, we will be using this library to make call to AWS APIs\n",
    "    - awsglue - AWS's pyspark library which provides the needed \n",
    "    \n",
    "    \n",
    "# Here data transform that we we will perform\n",
    "\n",
    "<img src=\"https://unnik.s3.amazonaws.com/public-files/unnik-lab-guides/aws-summit-2019/techfest/glue-generate-diagram.png\" alt=\"drawing\" width=\"600\"/>\n",
    "\n",
    "\n",
    "#### Execute Code ðŸ”»"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Spark application\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>User</th><th>Current session?</th></tr><tr><td>0</td><td>application_1654136102606_0001</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-172-38-189-57.ec2.internal:20888/proxy/application_1654136102606_0001/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-172-38-85-22.ec2.internal:8042/node/containerlogs/container_1654136102606_0001_01_000001/livy\">Link</a></td><td>None</td><td>âœ”</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession available as 'spark'.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "import sys\n",
    "from awsglue.transforms import *\n",
    "from awsglue.utils import getResolvedOptions\n",
    "from pyspark.context import SparkContext\n",
    "from awsglue.context import GlueContext\n",
    "from awsglue.job import Job\n",
    "import boto3\n",
    "import time\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploring your raw dataset\n",
    "- In this step you will:\n",
    "    - Create a dynamic frame for your 'raw' table from AWS Glue catalog\n",
    "    - Explore the schema of the datasets\n",
    "    - Count rows in raw table\n",
    "    - View a sample of the data \n",
    "\n",
    "## Glue Dynamic Frames Basics\n",
    "\n",
    "- AWS Glue's dynamic data frames is a powerful data structure.\n",
    "- They provide a precise representation of the underlying semi-structured data, especially when dealing with columns or fields with varying types.\n",
    "- They also provide powerful primitives to deal with nesting and unnesting.\n",
    "- A dynamic record is a self-describing record: Each record encodes its columns and types, so every record can have a schema that is unique from all others in the dynamic frame.\n",
    "- For ETL, we needed somthing more dynamic, hence we created the Glue Dynamic DataFrames. DDF are an implementaion of DF that relaxes the requiements of having a rigid schema. Its designed for semi-structured data.\n",
    "- It maintains a schema per-record, its easy to restucture, tag and modify. \n",
    "\n",
    "\n",
    "#### Read More : https://docs.aws.amazon.com/glue/latest/dg/aws-glue-api-crawler-pyspark-extensions-dynamic-frame.html\n",
    "\n",
    "#### Execute Code ðŸ”»\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "glueContext = GlueContext(SparkContext.getOrCreate())\n",
    "spark = glueContext.spark_session\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Crate dynamic frame from Glue catalog\n",
    "- In this block we are using gluecontext to create a new dynamicframe from glue catalog\n",
    "- \n",
    "\n",
    "Other ways to create dynamicframes in Glue:\n",
    "- create_dynamic_frame_from_rdd\n",
    "- create_dynamic_frame_from_catalog\n",
    "- create_dynamic_frame_from_options\n",
    "\n",
    "#### Read More:https://docs.aws.amazon.com/glue/latest/dg/aws-glue-api-crawler-pyspark-extensions-glue-context.html\n",
    "\n",
    "#### Execute Code ðŸ”»\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "raw_data = glueContext.create_dynamic_frame.from_catalog(database=\"analyticsworkshopdb\", table_name=\"raw\")\n",
    "\n",
    "reference_data = glueContext.create_dynamic_frame.from_catalog(database=\"analyticsworkshopdb\", table_name=\"reference_data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# View schema\n",
    "- In this step we view the schema of the dynamic frame\n",
    "- printSchema( ) â€“ Prints the schema of the underlying DataFrame.\n",
    "\n",
    "#### Execute Code ðŸ”»"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      "|-- uuid: string\n",
      "|-- device_ts: string\n",
      "|-- device_id: int\n",
      "|-- device_temp: int\n",
      "|-- track_id: int\n",
      "|-- activity_type: string\n",
      "|-- partition_0: string\n",
      "|-- partition_1: string\n",
      "|-- partition_2: string\n",
      "|-- partition_3: string"
     ]
    }
   ],
   "source": [
    "raw_data.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      "|-- track_id: string\n",
      "|-- track_name: string\n",
      "|-- artist_name: string"
     ]
    }
   ],
   "source": [
    "reference_data.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Count records\n",
    "- In this step we will count the number of records in the dataframe\n",
    "- count( ) â€“ Returns the number of rows in the underlying DataFrame\n",
    "\n",
    "#### Execute Code ðŸ”»"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "raw_data (count) = 14000\n",
      "reference_data (count) = 100"
     ]
    }
   ],
   "source": [
    "print(f'raw_data (count) = {raw_data.count()}')\n",
    "print(f'reference_data (count) = {reference_data.count()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Show sample records\n",
    "- You can use to method to show samples of data in the datasets\n",
    "- use show() method to display a sample of records in the frame\n",
    "- here were are showing the top 5 records in the DF\n",
    "\n",
    "\n",
    "#### Execute Code ðŸ”»"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+---------+-----------+--------+-------------+-----------+-----------+-----------+-----------+\n",
      "|                uuid|           device_ts|device_id|device_temp|track_id|activity_type|partition_0|partition_1|partition_2|partition_3|\n",
      "+--------------------+--------------------+---------+-----------+--------+-------------+-----------+-----------+-----------+-----------+\n",
      "|c02e3a10-9db1-496...|2022-06-02 01:45:...|       45|         40|      15|    Traveling|       2022|         06|         02|         01|\n",
      "|c6d3341e-1f2f-460...|2022-06-02 01:45:...|       37|         28|      13|    Traveling|       2022|         06|         02|         01|\n",
      "|08e87e81-1a57-421...|2022-06-02 01:45:...|       11|         32|      25|    Traveling|       2022|         06|         02|         01|\n",
      "|480d03af-4c8c-499...|2022-06-02 01:45:...|       38|         28|      21|    Traveling|       2022|         06|         02|         01|\n",
      "|a1085f7f-5c27-4a7...|2022-06-02 01:45:...|       45|         34|      19|    Traveling|       2022|         06|         02|         01|\n",
      "+--------------------+--------------------+---------+-----------+--------+-------------+-----------+-----------+-----------+-----------+\n",
      "only showing top 5 rows"
     ]
    }
   ],
   "source": [
    "raw_data.toDF().show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----------+--------------------+\n",
      "|track_id| track_name|         artist_name|\n",
      "+--------+-----------+--------------------+\n",
      "|       1| God's Plan|               Drake|\n",
      "|       2|Meant To Be|Bebe Rexha & Flor...|\n",
      "|       3|    Perfect|          Ed Sheeran|\n",
      "|       4|    Finesse|Bruno Mars & Cardi B|\n",
      "|       5|     Psycho|Post Malone Featu...|\n",
      "+--------+-----------+--------------------+\n",
      "only showing top 5 rows"
     ]
    }
   ],
   "source": [
    "reference_data.toDF().show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using Spark SQL to explore data\n",
    "\n",
    "- Having the ability of \n",
    "- In Glue, you can leverage Spark's SQL engine to run SQL queries over your data\n",
    "- If you have a DynamicFrame called my_dynamic_frame, you can use the following snippet to convert the DynamicFrame to a DataFrame, issue a SQL query, and then convert back to a DynamicFrame\n",
    "\n",
    "### Spark SQL - Filtering & Counting - activity_type = Running\n",
    "- In this block, we will filter & count the number of events with activity_type = Running\n",
    "\n",
    "#### Execute Code ðŸ”»"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running (count): 1376\n",
      "+--------------------+--------------------+---------+-----------+--------+-------------+-----------+-----------+-----------+-----------+\n",
      "|                uuid|           device_ts|device_id|device_temp|track_id|activity_type|partition_0|partition_1|partition_2|partition_3|\n",
      "+--------------------+--------------------+---------+-----------+--------+-------------+-----------+-----------+-----------+-----------+\n",
      "|29d6c9af-c4a7-4cb...|2022-06-02 01:45:...|       13|         32|      23|      Running|       2022|         06|         02|         01|\n",
      "|9b25e6b5-44be-4d8...|2022-06-02 01:45:...|       39|         34|      24|      Running|       2022|         06|         02|         01|\n",
      "|ae9ae1c3-6d68-422...|2022-06-02 01:45:...|       34|         28|      29|      Running|       2022|         06|         02|         01|\n",
      "|3ed7e3b9-9495-452...|2022-06-02 01:45:...|       41|         32|      19|      Running|       2022|         06|         02|         01|\n",
      "|6be5a500-15e8-41c...|2022-06-02 01:45:...|       20|         34|      12|      Running|       2022|         06|         02|         01|\n",
      "+--------------------+--------------------+---------+-----------+--------+-------------+-----------+-----------+-----------+-----------+\n",
      "only showing top 5 rows"
     ]
    }
   ],
   "source": [
    "# Adding raw_data as a temporary table in sql context for spark\n",
    "\n",
    "raw_data.toDF().createOrReplaceTempView(\"temp_raw_data\")\n",
    "\n",
    "# Running the SQL statement which \n",
    "runningDF = spark.sql(\"select * from temp_raw_data where activity_type = 'Running'\")\n",
    "print(f'Running (count): {runningDF.count()}')\n",
    "\n",
    "runningDF.show(5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spark SQL - Filtering & Counting - activity_type = Working\n",
    "- In this block, we will filter & count the number of events with activity_type = Working\n",
    "\n",
    "#### Execute Code ðŸ”»"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working (count): 2779\n",
      "+--------------------+--------------------+---------+-----------+--------+-------------+-----------+-----------+-----------+-----------+\n",
      "|                uuid|           device_ts|device_id|device_temp|track_id|activity_type|partition_0|partition_1|partition_2|partition_3|\n",
      "+--------------------+--------------------+---------+-----------+--------+-------------+-----------+-----------+-----------+-----------+\n",
      "|b664df51-555e-498...|2022-06-02 01:45:...|       29|         28|      26|      Working|       2022|         06|         02|         01|\n",
      "|10444bd1-dc5b-46e...|2022-06-02 01:45:...|       45|         40|      30|      Working|       2022|         06|         02|         01|\n",
      "|3fd228a3-84ec-410...|2022-06-02 01:45:...|       39|         32|      11|      Working|       2022|         06|         02|         01|\n",
      "|569d43db-38e5-4de...|2022-06-02 01:45:...|       26|         34|      26|      Working|       2022|         06|         02|         01|\n",
      "|b060f5f6-78e3-4c4...|2022-06-02 01:45:...|       44|         40|      26|      Working|       2022|         06|         02|         01|\n",
      "+--------------------+--------------------+---------+-----------+--------+-------------+-----------+-----------+-----------+-----------+\n",
      "only showing top 5 rows"
     ]
    }
   ],
   "source": [
    "# Running the SQL statement which \n",
    "workingDF = spark.sql(\"select * from temp_raw_data where activity_type = 'Working'\")\n",
    "print(f'Working (count): {workingDF.count()}')\n",
    "\n",
    "workingDF.show(5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Glue Transforms - Filtering & Counting - activity_type = Running\n",
    "- Now, lets perform the same operation using Glue inbuilt transforms\n",
    "- We will use the **filter** transform\n",
    "- Filter() - Selects records from a DynamicFrame and returns a filtered DynamicFrame.\n",
    "- You specify a function, such as a function, which determines whether a record is output (function returns true) or not (function returns false).\n",
    "- In this function, we are filtering on the condition activity_type == 'Running'\n",
    "\n",
    "#### Read More: https://docs.aws.amazon.com/glue/latest/dg/aws-glue-api-crawler-pyspark-transforms-filter.html#aws-glue-api-crawler-pyspark-transforms-filter-example\n",
    "\n",
    "#### Execute Code ðŸ”»"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running (count): 1376"
     ]
    }
   ],
   "source": [
    "\n",
    "def filter_function(dynamic_record):\n",
    "\tif dynamic_record['activity_type'] == 'Running':\n",
    "\t\treturn True\n",
    "\telse:\n",
    "\t\treturn False\n",
    "runningDF = Filter.apply(frame=raw_data, f=filter_function)\n",
    "\n",
    "print(f'Running (count): {runningDF.count()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Glue Transforms - Filtering & Counting - activity_type = Working (Using python Lambda Expressions)\n",
    "- Small anonymous functions can be created with the lambda keyword.\n",
    "- Lambda functions can be used wherever function objects are required. They are syntactically restricted to a single expression. \n",
    "- Example: This function returns the sum of its two arguments: lambda a, b: a+b.\n",
    "\n",
    "#### Execute Code ðŸ”»"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working (count): 2779"
     ]
    }
   ],
   "source": [
    "\n",
    "workingDF = Filter.apply(frame=raw_data, f=lambda x: x['activity_type'] == 'Working')\n",
    "\n",
    "print(f'Working (count): {workingDF.count()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Glue Transforms - Joining two dataframes \n",
    "- Performs an equality join on two DynamicFrames.\n",
    "- This transforms accepts the following arguments.\n",
    "    - frame1 â€“ The first DynamicFrame to join\n",
    "    - frame2 â€“ The second DynamicFrame to join\n",
    "    - keys1 â€“ The keys to join on for the first frame\n",
    "    - keys2 â€“ The keys to join on for the second frame\n",
    "- In our case we will be joining the these two frames : **raw_data** & **reference_data**\n",
    "- We will be joing these two frames on column **track_id**\n",
    "\n",
    "#### Read More: https://docs.aws.amazon.com/glue/latest/dg/aws-glue-api-crawler-pyspark-transforms-join.html\n",
    "\n",
    "#### Execute Code ðŸ”»"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "joined_data = Join.apply(raw_data, reference_data, 'track_id', 'track_id')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### View schema\n",
    "- In this step we view the schema of the dynamic frame\n",
    "- printSchema( ) â€“ Prints the schema of the underlying DataFrame.\n",
    "\n",
    "#### Execute Code ðŸ”»"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      "|-- track_id: string\n",
      "|-- partition_2: string\n",
      "|-- activity_type: string\n",
      "|-- .track_id: int\n",
      "|-- partition_1: string\n",
      "|-- device_temp: int\n",
      "|-- track_name: string\n",
      "|-- artist_name: string\n",
      "|-- partition_3: string\n",
      "|-- device_ts: string\n",
      "|-- device_id: int\n",
      "|-- partition_0: string\n",
      "|-- uuid: string"
     ]
    }
   ],
   "source": [
    "joined_data.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Cleaning up the joined_data dynamicframe\n",
    "- Other than the columns we were interested in we have the partition columns\n",
    "- These were generated by firehose for placing the files in yyyy/mm/dd/hh directory structure in S3\n",
    "- We will use Glue's in-built **DropFields** transform to drop partition columns\n",
    "\n",
    "###### Read more about AWS Glue transforms here : https://docs.aws.amazon.com/glue/latest/dg/built-in-transforms.html\n",
    "\n",
    "#### Execute Code ðŸ”»"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "joined_data_clean = DropFields.apply(frame=joined_data, paths=['partition_0','partition_1','partition_2','partition_3'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### View schema after DropFields transform\n",
    "#### Execute Code ðŸ”»"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      "|-- track_id: string\n",
      "|-- activity_type: string\n",
      "|-- .track_id: int\n",
      "|-- device_temp: int\n",
      "|-- track_name: string\n",
      "|-- artist_name: string\n",
      "|-- device_ts: string\n",
      "|-- device_id: int\n",
      "|-- uuid: string"
     ]
    }
   ],
   "source": [
    "joined_data_clean.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### sample data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------------+---------+-----------+----------+---------------+--------------------+---------+--------------------+\n",
      "|track_id|activity_type|.track_id|device_temp|track_name|    artist_name|           device_ts|device_id|                uuid|\n",
      "+--------+-------------+---------+-----------+----------+---------------+--------------------+---------+--------------------+\n",
      "|      21|    Traveling|       21|         28|   Him & I|G-Eazy & Halsey|2022-06-02 01:45:...|       38|480d03af-4c8c-499...|\n",
      "|      21|    Traveling|       21|         34|   Him & I|G-Eazy & Halsey|2022-06-02 01:45:...|       26|701c0e53-6db4-471...|\n",
      "|      21|    Traveling|       21|         32|   Him & I|G-Eazy & Halsey|2022-06-02 01:45:...|       30|c38e32e0-8e7a-43f...|\n",
      "|      21|    Traveling|       21|         28|   Him & I|G-Eazy & Halsey|2022-06-02 01:45:...|        4|a71ae55e-acc9-4f7...|\n",
      "|      21|      Sitting|       21|         40|   Him & I|G-Eazy & Halsey|2022-06-02 01:45:...|       29|56c983f1-05c1-443...|\n",
      "+--------+-------------+---------+-----------+----------+---------------+--------------------+---------+--------------------+\n",
      "only showing top 5 rows"
     ]
    }
   ],
   "source": [
    "joined_data_clean.toDF().show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final step of the transform - Writing transformed data to S3\n",
    "- In this step we will be using Glue's write_dynamic_frame functionality to write transformed data to S3\n",
    "- We will be storing the transformed data in a different directory & in parquet format\n",
    "- make sure you change the D3 bucket name **yourname-analytics-workshop-bucket** to reflect your bucket name \n",
    "\n",
    "\n",
    "---\n",
    "- Why parquet format ? \n",
    "    - Apache Parquet is a columnar storage formats that is optimized for fast retrieval of data and used in AWS analytical applications.\n",
    "    - Columnar storage formats have the following characteristics that make them suitable for using with Athena:\n",
    "    Compression by column, with compression algorithm selected for the column data type to save storage space in Amazon S3 and reduce disk space and I/O during query processing.\n",
    "    - Predicate pushdown in Parquet and ORC enables queries to fetch only the blocks it needs, improving query performance.\n",
    "    - When a  query obtains specific column values from your data, it uses statistics from data block predicates, such as max/min values, to determine whether to read or skip the block.\n",
    "    - Splitting of data in Parquet allows analytics tools to split the reading of data to multiple readers and increase parallelism during its query processing.\n",
    "    \n",
    "#### Execute Code ðŸ”»"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformed data written to S3"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    datasink = glueContext.write_dynamic_frame.from_options(\n",
    "        frame = joined_data_clean, connection_type=\"s3\",\n",
    "        connection_options = {\"path\": \"s3://kinesis-analytics-workshop-bucket/data/processed-data/\"},\n",
    "        format = \"parquet\")\n",
    "    print('Transformed data written to S3')\n",
    "except Exception as ex:\n",
    "    print('Something went wrong')\n",
    "    print(ex)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using boto3 to run & automate AWS Glue \n",
    "\n",
    "- Boto is the AWS SDK for Python. It enables Python developers to create, configure, and manage AWS services. Boto provides an easy to use, object-oriented API, as well as low-level access to AWS services.\n",
    "\n",
    "\n",
    "# Add transformed data set to glue catalog\n",
    "- Now they you have written your transformed data to S3, we need to add it to the glue catalog so you can query it using athena\n",
    "- This block of take take close to 60 seconds to run, do not terminate  stop the execution\n",
    "\n",
    "\n",
    "#### Execute Code ðŸ”»"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---\n",
      "Crawler Stopped\n",
      "---"
     ]
    }
   ],
   "source": [
    "\n",
    "glueclient = boto3.client('glue', region_name='us-east-1')\n",
    "\n",
    "response = glueclient.start_crawler(Name='AnalyticsworkshopCrawler')\n",
    "\n",
    "print('---')\n",
    "\n",
    "crawler_state = None\n",
    "while (crawler_state != 'STOPPING'):\n",
    "    response = glueclient.get_crawler(Name='AnalyticsworkshopCrawler')\n",
    "    crawler_state = str(response['Crawler']['State'])\n",
    "    time.sleep(1)\n",
    "\n",
    "print('Crawler Stopped')\n",
    "print('---')\n",
    "time.sleep(3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use boto to view the list of tables in analyticsworkshopdb database\n",
    "\n",
    "#### Execute Code ðŸ”»"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "** analyticsworkshopdb has following tables**\n",
      "508c54a2_1eac_4af9_a180_3b6d1a38baca_csv\n",
      "508c54a2_1eac_4af9_a180_3b6d1a38baca_csv_metadata\n",
      "processed_data\n",
      "raw\n",
      "reference_data"
     ]
    }
   ],
   "source": [
    "\n",
    "print('** analyticsworkshopdb has following tables**')\n",
    "response = glueclient.get_tables(\n",
    "    DatabaseName='analyticsworkshopdb',\n",
    ")\n",
    "\n",
    "for table in response['TableList']:\n",
    "    print(table['Name'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ðŸ˜Ž\n",
    "========================="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### If you wish you take this notebook and its output back home - you can download / export it:\n",
    "\n",
    "- In Jupyter's menu bar click **File**:\n",
    "    - Download As: Notebook(.ipynb) (you can reimport it a jupyter notebook in the future)\n",
    "    - Download As: HTML (shows code + results in an easy to read format)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "========================="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NEXT Steps: Go back to the lab guide"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Sparkmagic (PySpark)",
   "language": "python",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python3"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
